<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Xiachong Feng</title>

  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Xiachong Feng ÂÜØÂ§èÂÜ≤</name>
                  </p>
                  <p>
                    Currently, I am a Postdoc Fellow of <a href="https://hkunlp.github.io/">Natural Language
                      Processing Group, The University of Hong Kong (HKU NLP)</a>, working with <a
                      href="https://ikekonglp.github.io/index.html">Prof. Lingpeng Kong</a> and <a
                      href="https://i.cs.hku.hk/~cwu/">Prof. Chuan Wu.
                  </p>

                  <p>Previously, I am a Ph.D student from <a href="https://www.hit.edu.cn/">Harbin Institute of
                      Technology</a>,
                    where I am a member of <a href="http://ir.hit.edu.cn/2112.html">Text Generation Group</a> of <a
                      href="http://ir.hit.edu.cn/">HIT-SCIR Lab</a> under the supervision of <a
                      href="https://scholar.google.com/citations?user=LKnCub0AAAAJ&hl=zh-CN">Prof. Bing Qin</a> and <a
                      href="https://scholar.google.com/citations?user=Xu8NbhYAAAAJ&hl=zh-CN">Prof. Xiaocheng Feng</a>.
                  </p>


                  <p style="text-align:center">
                    <a href="mailto:xiachongfeng1996@gmail.com">Email</a> &nbsp/&nbsp
                    <a href="data/cv_for_xcfeng.pdf">CV</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com.hk/citations?user=Wifx6goAAAAJ&hl=en">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://github.com/xcfcode">Github</a> &nbsp/&nbsp
                    <a href="https://twitter.com/xc_feng">Twitter</a>
                  </p>

                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/xcfeng.jpg"><img style="width:80%;max-width:80%" alt="profile photo"
                      src="images/xcfeng.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <heading>Research Overview</heading>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <p>
                    I work on AI ü§ù Social Science.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <heading>Talk</heading>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <p>
                    <li> <span style="color:#B08519">Jul. 2024</span>, I gave a talk "<a
                        href="./res/presentation/ai-game.pdf" target="_blank">Strategic Reasoning of Large Language
                        Models from a Game Theory Perspective</a>"
                      at the CCF 2024 Annual Conference on Computational Economics.
                    </li>
                    <li> <span style="color:#B08519">Sep. 2023</span>, I gave a talk "<a
                        href="./res/presentation/SMP2023.pptx" target="_blank">Taking the First Step Toward
                        Interdisciplinary Studies</a>" at the 2023 China Conference on Social Media Processing.</li>
                    <li> <span style="color:#B08519">May 2023</span>, I gave a talk "<a
                        href="./res/presentation/LLMs.pdf" target="_blank">Recent Advances in
                        Larg
                        Language Models</a>" at Interactive Data
                      Exploration System (IDEAS) lab, Shandong University.</li>
                    <li> <span style="color:#B08519">Jan. 2022</span>, I gave a talk "<a
                        href="./res/presentation/Dialogue_Summarization_DAMO.pdf" target="_blank">Dialogue
                        Summarizations</a>" at the
                      Alibaba DAMO Academy, Language
                      Technology Lab, Conversational AI.</li>
                    <li> <span style="color:#B08519">Jun. 2021</span>, I gave a talk "<a
                        href="./res/presentation/Dialogue_Summarization.pdf" target="_blank">Dialogue
                        Summarizations</a>" at the
                      Natural Language Processing Lab at
                      Tsinghua University: THUNLP.</li>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>


          <heading>Selected Publication</heading>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Adapter-based Selective Knowledge Distillation for Federated Multi-domain Meeting
                    Summarization</papertitle>
                  </a>
                  <br>
                  <strong>Xiachong Feng</strong>, Xiaocheng Feng, Xiyuan Du, Min-Yen Kan, Bing Qin
                  <br>
                  <i>IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP)</i>
                  <br>
                  <a href="https://arxiv.org/abs/2308.03275">[paper]</a>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Aligning Semantic in Brain and Language: A Curriculum Contrastive Method for
                    Electroencephalography-to-Text Generation</papertitle>
                  </a>
                  <br>
                  <strong>Xiachong Feng</strong>, Xiaocheng Feng, Bing Qin, Ting Liu
                  <br>
                  <i>IEEE Transactions on Neural Systems and Rehabilitation Engineering (TNSRE)</i>
                  <br>
                  <a href="https://ieeexplore.ieee.org/abstract/document/10248031/">[paper]</a>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>A Survey on Dialogue Summarization: Recent Advances and New Frontiers</papertitle>
                  </a>
                  <br>
                  <strong>Xiachong Feng</strong>, Xiaocheng Feng, Bing Qin
                  <br>
                  <i>IJCAI 2022</i>
                  <br>
                  <a href="https://arxiv.org/abs/2107.03175">[paper]</a> <a
                    href="https://mp.weixin.qq.com/s/628OAOW1_-Yc_vQbeuY_uA"> [blog] </a>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Language Model as an Annotator: Exploring DialoGPT for Dialogue Summarization</papertitle>
                  </a>
                  <br>
                  <strong>Xiachong Feng</strong>, Xiaocheng Feng, Libo Qin, Bing Qin, Ting Liu
                  <br>
                  <i>ACL 2021</i>
                  <br>
                  <a href="https://aclanthology.org/2021.acl-long.117/">[paper]</a>
                  <a href="https://github.com/xcfcode/PLM_annotator">[code]</a>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Dialogue Discourse-Aware Graph Model and Data Augmentation for Meeting Summarization
                  </papertitle>
                  </a>
                  <br>
                  <strong>Xiachong Feng</strong>, Xiaocheng Feng, Bing Qin, Xinwei Geng
                  <br>
                  <i>IJCAI 2021</i>
                  <br>
                  <a href="https://www.ijcai.org/proceedings/2021/524">[paper]</a>
                  <a href="https://github.com/xcfcode/DDAMS">[code]</a>
                </td>
              </tr>

              <!-- <tr>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Incorporating Commonsense Knowledge into Abstractive Dialogue Summarization via
                    Heterogeneous Graph Networks
                  </papertitle>
                  </a>
                  <br>
                  <strong>Xiachong Feng</strong>, Xiaocheng Feng, Bing Qin
                  <br>
                  <i>CCL 2021, Best Paper Award</i>
                  <br>
                  <a href="https://aclanthology.org/2021.ccl-1.86/">[paper]</a>
                  <a href="https://github.com/xcfcode/DHGN">[code]</a>
                </td>
              </tr> -->
            </tbody>
          </table>

          <!-- <heading>Competitions</heading>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <p>
                    <li>ÂçÉË®ÄÔºöÁ¨¨‰∫åÂ±äÈù¢Âêë‰∫ãÂÆû‰∏ÄËá¥ÊÄßÁöÑÁîüÊàêËØÑÊµãÊØîËµõ. Rank 4/70
                    </li>
                    <li>ICASSP 2023 General Meeting Understanding and Generation
                      Challenge Track5: Action Item Detection (AID). Rank 1st.</li>
                  </p>
                </td>
              </tr>
            </tbody>
          </table> -->

          <heading>Awards</heading>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <p>
                    <li>National Scholarship for Ph.D., 2021</li>
                    <li>Outstanding Graduate Award at Provincial Level, 2018</li>
                    <li>Outstanding Student Cadres at Provincial Level, 2017</li>
                    <li>National Scholarship for B.E., 2017</li>
                    <li>National Scholarship for B.E., 2016</li>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <heading>Education</heading>
          <table table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>

              <td width="15%">
                <img src="images/hit_high.png" width="120">
              </td>

              <td style="padding:20px;width:75%;vertical-align:middle">
                <strong>Harbin Institute of Technology</strong><br>
                <br>
                ‚Ä¢ Ph.D in Computer Science <br>‚Ä¢ 2018.09 ~ 2023.12 <br><br>
                ‚Ä¢ B.E in Software Engineering <br>‚Ä¢ 2014.09 ~ 2018.06
              </td>
            </tbody>
          </table>

          <table table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>

              <td width="15%">
                <img src="images/nus.jpeg" width="120">
              </td>

              <td style="padding:20px;width:75%;vertical-align:middle">
                <strong>National University of Singapore</strong><br>
                <br>
                ‚Ä¢ Remote Intern <br>‚Ä¢ 2020.08 ~ 2023.06<br>
              </td>
            </tbody>
          </table>


          <heading>Presentation</heading>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <p>
                    <li>Recent Advances in Large
                      Languag Models <a href="./res/presentation/LLMs.pdf" target="_blank">[PDF]</a></li>
                    <li>ChatGPT Evaluation for
                      NLP: A Meta Survey <a href="./res/presentation/ChatGPT_for_NLP.pdf" target="_blank">[PDF]</a>
                      <a href="https://docs.google.com/presentation/d/1PnzlHl8qDra6S4Zuuftk_o4v7WWN0uZTb-I7uNwNwAY/edit?usp=sharing&resourcekey=0-xbkOuvWVlGZTbPnflAqrCQ"
                        target="_blank">[Google Doc]</a>
                    </li>
                    <li>Federated Learning Meets
                      Knowledge Distillation <a href="./res/presentation/FedDistill.pdf" target="_blank">[PDF]</a></li>
                    <li>Dialogue Summarization(2022.1) <a href="./res/presentation/Dialogue_Summarization_DAMO.pdf"
                        target="_blank">[PDF]</a></li>
                    <li>Prompt Survey: <a href="./res/presentation/Prompt-part1.pdf" target="_blank">[PDF]</a> <a
                        href="./res/presentation/Prompt-part1.pptx" target="_blank">[PPTX]</a></li>
                    <li>Dialogue Summarization(2021.5) <a href="./res/presentation/Dialogue_Summarization.pdf"
                        target="_blank">[PDF]</a>
                    </li>
                    <li>ÂØπËØùÊëòË¶ÅÊúÄÊñ∞ËøõÂ±ïÁÆÄËø∞ <a href="./res/presentation/Dialogue_Summarization.pdf" target="_blank">[PDF]</a>
                      <a href="https://mp.weixin.qq.com/s/628OAOW1_-Yc_vQbeuY_uA" target="_blank">[Blog]</a>
                    </li>
                    <li>Cross-lingual Summarization <a href="./res/presentation/Cross-lingual_Summarization.pdf"
                        target="_blank">[PDF]</a></li>
                    <li>Efficient Transformers <a href="./res/presentation/Efficient_Transformers.pdf"
                        target="_blank">[PDF]</a>
                    </li>
                    <li>Multi-modal Summarization <a href="./res/presentation/Multi-modal-Summarization.pdf"
                        target="_blank">[PDF]</a></li>
                    <li>ACL20 Summarization <a href="./res/presentation/acl2020-summarization.pdf"
                        target="_blank">[PDF]</a>
                    </li>
                    <li>Data Augmentation <a href="./res/presentation/Data Augmentation.pdf" target="_blank">[PDF]</a>
                    </li>
                    <li>ÊñáÊú¨ÊëòË¶ÅÁÆÄËø∞ <a href="/res/presentation/ÊñáÊú¨ÊëòË¶ÅÁÆÄËø∞.pdf" target="_blank">[PDF]</a></li>
                    <li>ACL19 Summarization <a href="./res/presentation/ACL19 Summarization.pdf"
                        target="_blank">[PDF]</a></li>
                    <li>Graph Neural Networks <a href="./res/presentation/Graph Neural Networks.pdf"
                        target="_blank">[PDF]</a>
                    </li>
                    <li>Knowledge Distillation <a href="./res/presentation/Knowledge Distillation.pdf"
                        target="_blank">[PDF]</a>
                    </li>
                    <li>Meta Learning <a href="./res/presentation/Meta Learning.pdf" target="_blank">[PDF]</a></li>
                    <li>Non-Autoregressive Decoding <a href="./res/presentation/Non-Autoregressive Decoding.pdf"
                        target="_blank">[PDF]</a></li>
                    <li>Event Extraction <a href="./res/presentation/Event Extraction.pdf" target="_blank">[PDF]</a>
                    </li>
                    <li>Advanced Pre-training Language Models: a Brief Introduction <a
                        href="./res/presentation/Advanced pre-training language models a brief introduction.pdf"
                        target="_blank">[PDF]</a></li>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>



          <heading>Paper Slides</heading>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <p>
                    <li> Language Models Don‚Äôt Always Say What They Think Unfaithful
                      Explanations in Chain-of-Thought Prompting <a
                        href="./res/paper-slides/Language Models Don‚Äôt Always Say What They Think Unfaithful Explanations in Chain-of-Thought Prompting.pdf"
                        target="_blank">[PDF]</a></li>
                    <li> Cognitive Architectures for Language Agents <a
                        href="./res/paper-slides/Cognitive Architectures for Language Agents.pdf"
                        target="_blank">[PDF]</a></li>

                    <li> Do Androids Laugh at Electric Sheep? Humor ‚ÄúUnderstanding‚Äù
                      Benchmarks from The New Yorker Caption Contest <a
                        href="./res/paper-slides/Do Androids Laugh at Electric Sheep Humor Understanding Benchmarks from The New Yorker Caption Contest.pdf"
                        target="_blank">[PDF]</a></li>
                    <li> Evidence of a predictive
                      coding hierarchy in the human brain listening to speech <a
                        href="https://docs.google.com/presentation/d/18Md2n4aq9_nZZPRPgEnCaVeAa0RnslktRErL87PS6aM/edit?usp=sharing&resourcekey=0-yIcktTYo_UeX4Gsjh2t0gQ"
                        target="_blank">[Google Doc]</a></li>
                    <li> BrainBERT: Self-supervised representation learning for Intracranial
                      Electrodes <a
                        href="./res/paper-slides/BrainBERT Self-supervised representation learning for Intracranial Electrodes.pdf"
                        target="_blank">[PDF]</a></li>
                    <li> switch-GLAT: Multilingual Parallel Machine Translation Via
                      Code-Switch Decoder <a href="./res/paper-slides/switch-GLAT.pdf" target="_blank">[PDF]</a>
                    </li>
                    <li> Memorizing Transformers <a href="./res/paper-slides/Memorizing_Transformers.pdf"
                        target="_blank">[PDF]</a></li>
                    <li> One Model, Multiple Tasks: Pathways for Natural Language
                      Understanding <a
                        href="./res/paper-slides/One Model, Multiple Tasks Pathways for Natural Language Understanding.pdf"
                        target="_blank">[PDF]</a></li>
                    <li> CPT Colorful Prompt Tuning for Pre-trained Vision-Language Models
                      <a href="./res/paper-slides/CPT Colorful Prompt Tuning for Pre-trained Vision-Language Models.pptx"
                        target="_blank">[PPTX]</a>
                    </li>
                    <li> Contrastive Learning with Adversarial Perturbations for Conditional
                      Text Generation <a
                        href="./res/paper-slides/Contrastive Learning with Adversarial Perturbations for Conditional Text Generation.pdf"
                        target="_blank">[PDF]</a></li>
                    <li> Making Pre-trained Language Models Better Few-shot Learners <a
                        href="./res/paper-slides/Making Pre-trained Language Models Better Few-shot Learners.pdf"
                        target="_blank">[PDF]</a></li>
                    <li> Group-wise Contrastive Learning for Neural Dialogue Generation <a
                        href="./res/paper-slides/group-wise-contrastive-learning-for-neural-dialogue-generation.pdf"
                        target="_blank">[PDF]</a></li>
                    <li> Heterogeneous Graph Transformer <a
                        href="./res/paper-slides/Heterogeneous-Graph-Transformer.pdf" target="_blank">[PDF]</a>
                    </li>
                    <li> Plug and Play Language Model- A Simple Baseline for Controlled
                      Language Generation <a
                        href="./res/paper-slides/Plug and Play Language Model- A Simple Baseline for Controlled Language Generation.pdf"
                        target="_blank">[PDF]</a></li>
                    <li> Multimodal Abstractive Summarization for How2 Videos <a
                        href="./res/paper-slides/Multimodal Abstractive Summarization for How2 Videos.pdf"
                        target="_blank">[PDF]</a></li>
                    <li> A Simple Theoretical Model of Importance for Summarization <a
                        href="./res/paper-slides/A Simple Theoretical Model of Importance for Summarization.pdf"
                        target="_blank">[PDF]</a></li>
                    <li> Text Generation from Knowledge Graphs with Graph Transformers <a
                        href="./res/paper-slides/Text Generation from Knowledge Graphs with Graph Transformers.pdf"
                        target="_blank">[PDF]</a></li>
                    <li> The Curious Case of Neural Text Degeneration <a
                        href="./res/paper-slides/The Curious Case of Neural Text Degeneration.pdf"
                        target="_blank">[PDF]</a>
                    </li>
                    <li> Episodic Memory in Lifelong Language Learning <a
                        href="./res/paper-slides/Episodic Memory in Lifelong Language Learning.pdf"
                        target="_blank">[PDF]</a>
                    </li>
                    <li> DialogueGCN-A Graph Convolutional Neural Network for Emotion
                      Recognition in Conversation <a
                        href="./res/paper-slides/DialogueGCN-A Graph Convolutional Neural Network for Emotion Recognition in Conversation.pdf"
                        target="_blank">[PDF]</a></li>
                    <li> Commonsense for Generative Multi-Hop Question Answering Tasks <a
                        href="./res/paper-slides/Commonsense for Generative Multi-Hop Question Answering Tasks.pdf"
                        target="_blank">[PDF]</a></li>
                    <li> Commonsense Knowledge Aware Conversation Generation with Graph
                      Attention <a
                        href="./res/paper-slides/Commonsense Knowledge Aware Conversation Generation with Graph Attention.pdf"
                        target="_blank">[PDF]</a></li>
                    <li> Deep Multitask Learning for Semantic Dependency Parsing <a
                        href="./res/paper-slides/Deep Multitask Learning for Semantic Dependency Parsing.pdf"
                        target="_blank">[PDF]</a></li>
                    <li> Dynamically Fused Graph Network for Multi-hop Reasoning <a
                        href="./res/paper-slides/Dynamically Fused Graph Network for Multi-hop Reasoning.pdf"
                        target="_blank">[PDF]</a></li>
                    <li> Emotional Chatting Machine Emotional Conversation Generation with
                      Internal and External
                      Memory <a
                        href="./res/paper-slides/Emotional Chatting Machine Emotional Conversation Generation with Internal and External Memory.pdf"
                        target="_blank">[PDF]</a></li>
                    <li> Learning Neural Templates for Text Generation <a
                        href="./res/paper-slides/Learning Neural Templates for Text Generation.pdf"
                        target="_blank">[PDF]</a>
                    </li>
                    <li> Learning to Ask Questions in Open-domain Conversational Systems
                      with Typed Decoders <a
                        href="./res/paper-slides/Learning to Ask Questions in Open-domain Conversational Systems with Typed Decoders .pdf"
                        target="_blank">[PDF]</a></li>
                    <li> Linguistic Knowledge and Transferability of Contextual
                      Representations <a
                        href="./res/paper-slides/Linguistic Knowledge and Transferability of Contextual Representations.pdf"
                        target="_blank">[PDF]</a></li>
                    <li> Multi-Domain Neural Machine Translation with Word-Level Domain
                      Context Discrimination <a
                        href="./res/paper-slides/Multi-Domain Neural Machine Translation with Word-Level Domain Context Discrimination.pdf"
                        target="_blank">[PDF]</a></li>
                    <li> Semi-Supervised QA with Generative Domain-Adaptive Nets <a
                        href="./res/paper-slides/Semi-Supervised QA with Generative Domain-Adaptive Nets.pdf"
                        target="_blank">[PDF]</a></li>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>


          <heading>Notes</heading>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <p>

                    <li> EMNLP19 Summarization <a href="./res/notes/EMNLP19_Summarization.pdf" target="_blank">[PDF]</a>
                    </li>
                    <li> Cross-Lingual Word Embedding <a href="./res/notes/x-lingual-v1.0.pdf" target="_blank">[PDF]</a>
                    </li>
                    <li> Brief Intro to Summarization <a href="./res/notes/Brief-intro-to-summarization.pdf"
                        target="_blank">[PDF]</a>
                    </li>
                    <li> GAN in Text Generation <a href="./res/notes/GAN%20in%20Text%20Generation.pdf"
                        target="_blank">[PDF]</a></li>
                    <li> EMNLP19 and NIPS19 Notes <a href="./res/notes/EMNLP19_and_NIPS19_Notes.pdf"
                        target="_blank">[PDF]</a></li>
                    <li> Boosting <a href="./res/notes/Boosting.pdf" target="_blank">[PDF]</a></li>
                    <li> The Maximum Entropy Model <a href="./res/notes/The%20Maximum%20Entropy%20Model.pdf"
                        target="_blank">[PDF]</a>
                    </li>
                    <li> HMM <a href="./res/notes/HMM.pdf" target="_blank">[PDF]</a></li>
                    <li> ConceptNet <a href="./res/notes/ConceptNet.pdf" target="_blank">[PDF]</a>
                    </li>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>


          <table table width="25%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>

              <td width="100%">
                <script type="text/javascript" id="clstr_globe"
                  src="//clustrmaps.com/globe.js?d=lWxEXypJOAXGaGtAnKrTVi-6bF1z4ItcPTfRlLtZMQY"></script>
              </td>

            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    Design and source code from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
                </td>
              </tr>
            </tbody>
          </table>




        </td>
      </tr>
  </table>
</body>

</html>